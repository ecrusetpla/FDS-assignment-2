{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./fds-link-prediction-madhura/archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes = pd.read_csv(f\"{data_path}/attributes.csv\")\n",
    "df_predictions = pd.read_csv(f\"{data_path}/attributes.csv\")\n",
    "df_solutionInput = pd.read_csv(f\"{data_path}/solutionInput.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data structure so it can be use in the model\n",
    "solution_edges = df_solutionInput[['int1', 'int2']].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graphs\n",
    "G = nx.read_edgelist(f\"{data_path}/edges_train.edgelist\", delimiter=',', nodetype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(G, i, j):\n",
    "    features = []\n",
    "    \n",
    "    # Preferential attachment\n",
    "    pa = len(list(G.neighbors(i))) * len(list(G.neighbors(j)))\n",
    "    features.append(pa)\n",
    "    \n",
    "    # Common neighbors\n",
    "    common_neighbors = len(list(nx.common_neighbors(G, i, j)))\n",
    "    features.append(common_neighbors)\n",
    "\n",
    "    # Proportion of common neighbours\n",
    "    proportion_common_neighbours = len(list(nx.common_neighbors(G, i, j))) / (len(list(G.neighbors(i))) + len(list(G.neighbors(j))))\n",
    "    features.append(proportion_common_neighbours)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard = list(nx.jaccard_coefficient(G, [(i, j)]))[0][2]\n",
    "    features.append(jaccard)\n",
    "\n",
    "    # Adamic-Adar index\n",
    "    adamic_ajar = list(nx.adamic_adar_index(G, [(i, j)]))[0][2]\n",
    "    features.append(adamic_ajar)\n",
    "\n",
    "    # Resource allocation index \n",
    "    ra_index = list(nx.resource_allocation_index(G, [(i, j)]))[0][2]\n",
    "    features.append(ra_index)\n",
    "\n",
    "    # Resouece allocation index Soundarajan-Hopcroft\n",
    "    rash_index = list(nx.ra_index_soundarajan_hopcroft(G, [(i, j)], 'community'))[0][2]\n",
    "    features.append(rash_index)\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(G, i, j):\n",
    "    features = []\n",
    "    \n",
    "    # Store neighbor sets for reuse\n",
    "    neighbors_i = set(G.neighbors(i))\n",
    "    neighbors_j = set(G.neighbors(j))\n",
    "    len_neighbors_i = len(neighbors_i)\n",
    "    len_neighbors_j = len(neighbors_j)\n",
    "    \n",
    "    # Existing features\n",
    "    # Preferential attachment\n",
    "    pa = len_neighbors_i * len_neighbors_j\n",
    "    features.append(pa)\n",
    "    \n",
    "    # Common neighbors\n",
    "    common_neighbors_set = neighbors_i.intersection(neighbors_j)\n",
    "    common_neighbors = len(common_neighbors_set)\n",
    "    features.append(common_neighbors)\n",
    "\n",
    "    # Proportion of common neighbours\n",
    "    try:\n",
    "        proportion_common_neighbours = common_neighbors / (len_neighbors_i + len_neighbors_j - common_neighbors)\n",
    "    except ZeroDivisionError:\n",
    "        proportion_common_neighbours = 0\n",
    "    features.append(proportion_common_neighbours)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard = list(nx.jaccard_coefficient(G, [(i, j)]))[0][2]\n",
    "    features.append(jaccard)\n",
    "\n",
    "    # Adamic-Adar index\n",
    "    adamic_adar = list(nx.adamic_adar_index(G, [(i, j)]))[0][2]\n",
    "    features.append(adamic_adar)\n",
    "\n",
    "    # Resource allocation index \n",
    "    ra_index = list(nx.resource_allocation_index(G, [(i, j)]))[0][2]\n",
    "    features.append(ra_index)\n",
    "\n",
    "    # New features\n",
    "    \n",
    "    # 1. Shortest path length (if not connected)\n",
    "    # try:\n",
    "    #     shortest_path = nx.shortest_path_length(G, i, j)\n",
    "    # except nx.NetworkXNoPath:\n",
    "    #     shortest_path = -1  # or some other default value\n",
    "    # features.append(shortest_path)\n",
    "    \n",
    "    # 2. Local clustering coefficients\n",
    "    clustering_i = nx.clustering(G, i)\n",
    "    clustering_j = nx.clustering(G, j)\n",
    "    features.append(clustering_i)\n",
    "    features.append(clustering_j)\n",
    "    \n",
    "    # 3. Degree centrality difference\n",
    "    degree_centrality_diff = abs(nx.degree_centrality(G)[i] - nx.degree_centrality(G)[j])\n",
    "    features.append(degree_centrality_diff)\n",
    "    \n",
    "    # 4. Average neighbor degree\n",
    "    avg_neighbor_degree_i = sum(len(list(G.neighbors(n))) for n in neighbors_i) / len_neighbors_i if len_neighbors_i > 0 else 0\n",
    "    avg_neighbor_degree_j = sum(len(list(G.neighbors(n))) for n in neighbors_j) / len_neighbors_j if len_neighbors_j > 0 else 0\n",
    "    features.append(avg_neighbor_degree_i)\n",
    "    features.append(avg_neighbor_degree_j)\n",
    "    \n",
    "    # 5. Common neighbor two-hop\n",
    "    two_hop_neighbors_i = set().union(*[set(G.neighbors(n)) for n in neighbors_i]) if len_neighbors_i > 0 else set()\n",
    "    two_hop_neighbors_j = set().union(*[set(G.neighbors(n)) for n in neighbors_j]) if len_neighbors_j > 0 else set()\n",
    "    common_two_hop = len(two_hop_neighbors_i.intersection(two_hop_neighbors_j))\n",
    "    features.append(common_two_hop)\n",
    "    \n",
    "    # 6. Cosine similarity of neighbor sets\n",
    "    if len_neighbors_i > 0 and len_neighbors_j > 0:\n",
    "        cosine_sim = common_neighbors / (math.sqrt(len_neighbors_i) * math.sqrt(len_neighbors_j))\n",
    "    else:\n",
    "        cosine_sim = 0\n",
    "    features.append(cosine_sim)\n",
    "    \n",
    "    # 7. Hub and authority scores\n",
    "    hub_auth = nx.hits(G)[0]\n",
    "    hub_score_diff = abs(hub_auth[i] - hub_auth[j])\n",
    "    features.append(hub_score_diff)\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_edge_features(graph, edges):\n",
    "    features = []\n",
    "    for (i, j) in edges:\n",
    "        edge_features = getFeature(graph, i, j)\n",
    "        features.append(edge_features)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for the edges in the data set\n",
    "edges = list(G.edges())\n",
    "X = extract_edge_features(G, edges)\n",
    "y = [1] * len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.16800000e+03, 1.60000000e+01, 1.63265306e-01, ...,\n",
       "        2.38000000e+02, 2.84267622e-01, 1.96612198e-03],\n",
       "       [9.60000000e+02, 3.00000000e+00, 4.61538462e-02, ...,\n",
       "        1.58000000e+02, 9.68245837e-02, 3.12608744e-03],\n",
       "       [3.40800000e+03, 1.50000000e+01, 1.44230769e-01, ...,\n",
       "        2.23000000e+02, 2.56945766e-01, 2.10118749e-03],\n",
       "       ...,\n",
       "       [3.50000000e+01, 2.00000000e+00, 2.00000000e-01, ...,\n",
       "        3.60000000e+01, 3.38061702e-01, 5.79988225e-05],\n",
       "       [2.10000000e+01, 1.00000000e+00, 1.11111111e-01, ...,\n",
       "        1.90000000e+01, 2.18217890e-01, 1.04728736e-04],\n",
       "       [5.00000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.10000000e+01, 0.00000000e+00, 1.00715401e-04]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative samples for the data set\n",
    "def generate_negative_samples(graph, num_samples):\n",
    "    nodes = list(graph.nodes())\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        u, v = np.random.choice(nodes, size=2, replace=False)\n",
    "        if not graph.has_edge(u, v):\n",
    "            negative_samples.append((u, v))\n",
    "    return negative_samples\n",
    "\n",
    "num_negative_samples = int(len(edges)*1.00)  # % of negative samples\n",
    "negative_edges = generate_negative_samples(G, num_negative_samples)\n",
    "X_negative = extract_edge_features(G, negative_edges)\n",
    "y_negative = [0] * len(negative_edges)\n",
    "\n",
    "# Combine positive and negative samples for data set\n",
    "X = np.vstack((X, X_negative))\n",
    "y = np.concatenate((y, y_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.62000000e+02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        7.50000000e+01, 0.00000000e+00, 4.57052642e-04],\n",
       "       [5.00000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.60000000e+01, 0.00000000e+00, 1.15746189e-04],\n",
       "       [2.40000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.40000000e+01, 0.00000000e+00, 1.63108613e-04],\n",
       "       ...,\n",
       "       [7.00000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.50000000e+01, 0.00000000e+00, 5.90660100e-04],\n",
       "       [4.80000000e+01, 1.00000000e+00, 7.69230769e-02, ...,\n",
       "        1.60000000e+01, 1.44337567e-01, 3.91658766e-06],\n",
       "       [8.50000000e+01, 1.00000000e+00, 4.76190476e-02, ...,\n",
       "        2.20000000e+01, 1.08465229e-01, 8.27593762e-04]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, penalty='l2', C=1.5, solver='lbfgs', max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=5)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.90 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest model\n",
    "rf_clf = RandomForestClassifier(random_state=0, n_estimators=100, max_depth=10)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "rf_scores = cross_val_score(rf_clf, X_test, y_test, cv=5)\n",
    "print(\"Random Forest: %0.2f accuracy with a standard deviation of %0.2f\" % (rf_scores.mean(), rf_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.89 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNN model\n",
    "knn_clf = KNeighborsClassifier(n_neighbors= 20)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "knn_scores = cross_val_score(knn_clf, X_test, y_test, cv=5)\n",
    "print(\"KNN: %0.2f accuracy with a standard deviation of %0.2f\" % (knn_scores.mean(), knn_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting: 0.90 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting model\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    random_state=0, \n",
    "    n_estimators=1000, \n",
    "    learning_rate=0.01, \n",
    "    max_depth=5,  # Increased from 3\n",
    "    min_samples_leaf=10,  # Helps prevent overfitting\n",
    "    subsample=0.8  # Use 80% of samples for each tree, adds randomness\n",
    ")\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "gb_scores = cross_val_score(gb_clf, X_test, y_test, cv=5)\n",
    "print(\"Gradient Boosting: %0.2f accuracy with a standard deviation of %0.2f\" % (gb_scores.mean(), gb_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predictions: 643\n"
     ]
    }
   ],
   "source": [
    "\n",
    "solution_edges = df_solutionInput[['int1', 'int2']].astype(str).values.tolist()\n",
    "X_solution = extract_edge_features(G, solution_edges)\n",
    "\n",
    "predictions = rf_clf.predict(X_solution)\n",
    "\n",
    "df_solutionInput['prediction'] = predictions\n",
    "\n",
    "#df_solutionInput.to_csv('predicted_solution.csv', index=False)\n",
    "\n",
    "df_solutionInput.head()\n",
    "\n",
    "num_positive_predictions = df_solutionInput['prediction'].sum()\n",
    "\n",
    "print(f\"Number of positive predictions: {num_positive_predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predictions: 666\n"
     ]
    }
   ],
   "source": [
    "predictions = gb_clf.predict(X_solution)\n",
    "\n",
    "df_solutionInput['prediction'] = predictions\n",
    "\n",
    "#df_solutionInput.to_csv('predicted_solution.csv', index=False)\n",
    "\n",
    "df_solutionInput.head()\n",
    "\n",
    "num_positive_predictions = df_solutionInput['prediction'].sum()\n",
    "\n",
    "print(f\"Number of positive predictions: {num_positive_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities instead of class predictions\n",
    "prediction_probabilities = gb_clf.predict_proba(X_solution)\n",
    "\n",
    "# Get the probabilities for the positive class (class 1)\n",
    "positive_class_probabilities = prediction_probabilities[:, 1]\n",
    "\n",
    "# Find the threshold that gives exactly 50% positive predictions\n",
    "# This is the median of the probabilities\n",
    "threshold = np.median(positive_class_probabilities)\n",
    "\n",
    "# Make predictions using this threshold\n",
    "adjusted_predictions = (positive_class_probabilities >= threshold).astype(int)\n",
    "\n",
    "# Update the predictions in your DataFrame\n",
    "df_solutionInput['prediction'] = adjusted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_solutionInput['prediction'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solutionInput[[\"prediction\"]].to_csv(\"./attempt_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [ 0  6  7  9 10 11 13]\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 20, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Best cross-validation score: 0.8981060606060606\n",
      "Ensemble Model - Mean CV Score: 0.9061 (+/- 0.0105)\n",
      "Number of positive predictions: 733\n",
      "Proportion of positive predictions: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Feature Selection\n",
    "selector = SelectFromModel(estimator=RandomForestClassifier(n_estimators=100, random_state=42), threshold='median')\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "print(f\"Selected features: {selected_feature_indices}\")\n",
    "\n",
    "# 2. Hyperparameter Tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'subsample': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# 3. Ensemble Method\n",
    "rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "gb_clf_best = GradientBoostingClassifier(**grid_search.best_params_, random_state=42)\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('gb', gb_clf_best)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# 4. Final Evaluation using Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(ensemble_clf, X_selected, y, cv=skf, n_jobs=-1)\n",
    "\n",
    "print(f\"Ensemble Model - Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# 5. Train final model and predict\n",
    "ensemble_clf.fit(X_selected, y)\n",
    "\n",
    "# Transform solution data using the same feature selection\n",
    "X_solution_selected = selector.transform(X_solution)\n",
    "\n",
    "# Get predicted probabilities\n",
    "pred_probs = ensemble_clf.predict_proba(X_solution_selected)[:, 1]\n",
    "\n",
    "# Ensure exactly 50% positive predictions\n",
    "threshold = np.median(pred_probs)\n",
    "final_predictions = (pred_probs >= threshold).astype(int)\n",
    "\n",
    "# Update DataFrame and save predictions\n",
    "df_solutionInput['prediction'] = final_predictions\n",
    "df_solutionInput[['prediction']].to_csv(\"improved_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"Number of positive predictions: {final_predictions.sum()}\")\n",
    "print(f\"Proportion of positive predictions: {final_predictions.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solutionInput[['prediction']].to_csv(\"improved_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.9064 (+/- 0.0026)\n",
      "Mean AUC: 0.9636 (+/- 0.0019)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def robust_cross_validation(X, y, model, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    auc_scores = []\n",
    "    \n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_val, y_pred))\n",
    "        auc_scores.append(roc_auc_score(y_val, y_pred_proba))\n",
    "    \n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies):.4f} (+/- {np.std(accuracies):.4f})\")\n",
    "    print(f\"Mean AUC: {np.mean(auc_scores):.4f} (+/- {np.std(auc_scores):.4f})\")\n",
    "\n",
    "# Use this function with your best model\n",
    "best_model = GradientBoostingClassifier(**grid_search.best_params_, random_state=42)\n",
    "robust_cross_validation(X, y, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_two_hop: 0.8160\n",
      "avg_neighbor_degree_j: 0.0508\n",
      "preferential_attachment: 0.0490\n",
      "clustering_i: 0.0217\n",
      "avg_neighbor_degree_i: 0.0169\n",
      "hub_score_diff: 0.0124\n",
      "clustering_j: 0.0107\n",
      "degree_centrality_diff: 0.0074\n",
      "cosine_sim: 0.0050\n",
      "resource_allocation: 0.0044\n",
      "adamic_adar: 0.0034\n",
      "jaccard: 0.0015\n",
      "proportion_common_neighbours: 0.0008\n",
      "common_neighbors: 0.0002\n",
      "\n",
      "Selected features (RFE):\n",
      "['preferential_attachment', 'resource_allocation', 'clustering_i', 'clustering_j', 'degree_centrality_diff', 'avg_neighbor_degree_i', 'avg_neighbor_degree_j', 'common_two_hop', 'cosine_sim', 'hub_score_diff']\n",
      "\n",
      "Optimal number of features: 9\n",
      "Selected features (RFECV):\n",
      "['preferential_attachment', 'resource_allocation', 'clustering_i', 'clustering_j', 'degree_centrality_diff', 'avg_neighbor_degree_i', 'avg_neighbor_degree_j', 'common_two_hop', 'hub_score_diff']\n",
      "Mean Accuracy: 0.9055 (+/- 0.0035)\n",
      "Mean AUC: 0.9636 (+/- 0.0021)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "feature_names = [\n",
    "    'preferential_attachment',\n",
    "    'common_neighbors',\n",
    "    'proportion_common_neighbours',\n",
    "    'jaccard',\n",
    "    'adamic_adar',\n",
    "    'resource_allocation',\n",
    "    'clustering_i',\n",
    "    'clustering_j',\n",
    "    'degree_centrality_diff',\n",
    "    'avg_neighbor_degree_i',\n",
    "    'avg_neighbor_degree_j',\n",
    "    'common_two_hop',\n",
    "    'cosine_sim',\n",
    "    'hub_score_diff'\n",
    "]\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_importance = sorted(zip(feature_importance, feature_names), reverse=True)\n",
    "for importance, feature in feature_importance:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "rfe = RFE(estimator=best_model, n_features_to_select=10, step=1)\n",
    "rfe = rfe.fit(X, y)\n",
    "print(\"\\nSelected features (RFE):\")\n",
    "print([feature for feature, selected in zip(feature_names, rfe.support_) if selected])\n",
    "\n",
    "# Recursive Feature Elimination with Cross-Validation\n",
    "rfecv = RFECV(estimator=best_model, step=1, cv=5, scoring='accuracy')\n",
    "rfecv = rfecv.fit(X, y)\n",
    "print(f\"\\nOptimal number of features: {rfecv.n_features_}\")\n",
    "print(\"Selected features (RFECV):\")\n",
    "print([feature for feature, selected in zip(feature_names, rfecv.support_) if selected])\n",
    "\n",
    "# Train and evaluate model with selected features\n",
    "X_selected = rfecv.transform(X)\n",
    "X_solution_selected = rfecv.transform(X_solution)\n",
    "robust_cross_validation(X_selected, y, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.9068 (+/- 0.0056)\n",
      "Mean AUC: 0.9634 (+/- 0.0021)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = [\n",
    "    ('gb', GradientBoostingClassifier(**grid_search.best_params_, random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=1000, random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=1000, random_state=42)),\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "robust_cross_validation(X_selected, y, ensemble)\n",
    "\n",
    "# Final predictions\n",
    "scaler = StandardScaler()\n",
    "X_selected_scaled = scaler.fit_transform(X_selected)\n",
    "X_solution_selected_scaled = scaler.transform(X_solution_selected)\n",
    "\n",
    "ensemble.fit(X_selected_scaled, y)\n",
    "pred_probs = ensemble.predict_proba(X_solution_selected_scaled)[:, 1]\n",
    "\n",
    "# Ensure exactly 50% positive predictions\n",
    "threshold = np.median(pred_probs)\n",
    "final_predictions = (pred_probs >= threshold).astype(int)\n",
    "\n",
    "df_solutionInput['prediction'] = final_predictions\n",
    "df_solutionInput[['prediction']].to_csv(\"ensemble_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n"
     ]
    }
   ],
   "source": [
    "prediction_not_adjusted = ensemble.predict(X_solution_selected_scaled)\n",
    "df_solutionInput['prediction'] = prediction_not_adjusted\n",
    "print(df_solutionInput['prediction'].sum())\n",
    "df_solutionInput[['prediction']].to_csv(\"ensemble_predictions_not_adjusted.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
