{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./fds-link-prediction-madhura/archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes = pd.read_csv(f\"{data_path}/attributes.csv\")\n",
    "df_predictions = pd.read_csv(f\"{data_path}/attributes.csv\")\n",
    "df_solutionInput = pd.read_csv(f\"{data_path}/solutionInput.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data structure so it can be use in the model\n",
    "solution_edges = df_solutionInput[['int1', 'int2']].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graphs\n",
    "G = nx.read_edgelist(f\"{data_path}/edges_train.edgelist\", delimiter=',', nodetype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(G, i, j):\n",
    "    features = []\n",
    "    \n",
    "    # Preferential attachment\n",
    "    pa = len(list(G.neighbors(i))) * len(list(G.neighbors(j)))\n",
    "    features.append(pa)\n",
    "    \n",
    "    # Common neighbors\n",
    "    common_neighbors = len(list(nx.common_neighbors(G, i, j)))\n",
    "    features.append(common_neighbors)\n",
    "\n",
    "    # Proportion of common neighbours\n",
    "    proportion_common_neighbours = len(list(nx.common_neighbors(G, i, j))) / (len(list(G.neighbors(i))) + len(list(G.neighbors(j))))\n",
    "    features.append(proportion_common_neighbours)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard = list(nx.jaccard_coefficient(G, [(i, j)]))[0][2]\n",
    "    features.append(jaccard)\n",
    "\n",
    "    # Adamic-Adar index\n",
    "    adamic_ajar = list(nx.adamic_adar_index(G, [(i, j)]))[0][2]\n",
    "    features.append(adamic_ajar)\n",
    "\n",
    "    # Resource allocation index \n",
    "    ra_index = list(nx.resource_allocation_index(G, [(i, j)]))[0][2]\n",
    "    features.append(ra_index)\n",
    "\n",
    "    # Resouece allocation index Soundarajan-Hopcroft\n",
    "    rash_index = list(nx.ra_index_soundarajan_hopcroft(G, [(i, j)], 'community'))[0][2]\n",
    "    features.append(rash_index)\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(G, i, j):\n",
    "    features = []\n",
    "    \n",
    "    # Store neighbor sets for reuse\n",
    "    neighbors_i = set(G.neighbors(i))\n",
    "    neighbors_j = set(G.neighbors(j))\n",
    "    len_neighbors_i = len(neighbors_i)\n",
    "    len_neighbors_j = len(neighbors_j)\n",
    "    \n",
    "    # Existing features\n",
    "    # Preferential attachment\n",
    "    pa = len_neighbors_i * len_neighbors_j\n",
    "    features.append(pa)\n",
    "    \n",
    "    # Common neighbors\n",
    "    common_neighbors_set = neighbors_i.intersection(neighbors_j)\n",
    "    common_neighbors = len(common_neighbors_set)\n",
    "    features.append(common_neighbors)\n",
    "\n",
    "    # Proportion of common neighbours\n",
    "    try:\n",
    "        proportion_common_neighbours = common_neighbors / (len_neighbors_i + len_neighbors_j - common_neighbors)\n",
    "    except ZeroDivisionError:\n",
    "        proportion_common_neighbours = 0\n",
    "    features.append(proportion_common_neighbours)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard = list(nx.jaccard_coefficient(G, [(i, j)]))[0][2]\n",
    "    features.append(jaccard)\n",
    "\n",
    "    # Adamic-Adar index\n",
    "    adamic_adar = list(nx.adamic_adar_index(G, [(i, j)]))[0][2]\n",
    "    features.append(adamic_adar)\n",
    "\n",
    "    # Resource allocation index \n",
    "    ra_index = list(nx.resource_allocation_index(G, [(i, j)]))[0][2]\n",
    "    features.append(ra_index)\n",
    "\n",
    "    # New features\n",
    "    \n",
    "    # 1. Shortest path length (if not connected)\n",
    "    # try:\n",
    "    #     shortest_path = nx.shortest_path_length(G, i, j)\n",
    "    # except nx.NetworkXNoPath:\n",
    "    #     shortest_path = -1  # or some other default value\n",
    "    # features.append(shortest_path)\n",
    "    \n",
    "    # 2. Local clustering coefficients\n",
    "    clustering_i = nx.clustering(G, i)\n",
    "    clustering_j = nx.clustering(G, j)\n",
    "    features.append(clustering_i)\n",
    "    features.append(clustering_j)\n",
    "    \n",
    "    # 3. Degree centrality difference\n",
    "    degree_centrality_diff = abs(nx.degree_centrality(G)[i] - nx.degree_centrality(G)[j])\n",
    "    features.append(degree_centrality_diff)\n",
    "    \n",
    "    # 4. Average neighbor degree\n",
    "    avg_neighbor_degree_i = sum(len(list(G.neighbors(n))) for n in neighbors_i) / len_neighbors_i if len_neighbors_i > 0 else 0\n",
    "    avg_neighbor_degree_j = sum(len(list(G.neighbors(n))) for n in neighbors_j) / len_neighbors_j if len_neighbors_j > 0 else 0\n",
    "    features.append(avg_neighbor_degree_i)\n",
    "    features.append(avg_neighbor_degree_j)\n",
    "    \n",
    "    # 5. Common neighbor two-hop\n",
    "    two_hop_neighbors_i = set().union(*[set(G.neighbors(n)) for n in neighbors_i]) if len_neighbors_i > 0 else set()\n",
    "    two_hop_neighbors_j = set().union(*[set(G.neighbors(n)) for n in neighbors_j]) if len_neighbors_j > 0 else set()\n",
    "    common_two_hop = len(two_hop_neighbors_i.intersection(two_hop_neighbors_j))\n",
    "    features.append(common_two_hop)\n",
    "    \n",
    "    # 6. Cosine similarity of neighbor sets\n",
    "    if len_neighbors_i > 0 and len_neighbors_j > 0:\n",
    "        cosine_sim = common_neighbors / (math.sqrt(len_neighbors_i) * math.sqrt(len_neighbors_j))\n",
    "    else:\n",
    "        cosine_sim = 0\n",
    "    features.append(cosine_sim)\n",
    "    \n",
    "    # 7. Hub and authority scores\n",
    "    hub_auth = nx.hits(G)[0]\n",
    "    hub_score_diff = abs(hub_auth[i] - hub_auth[j])\n",
    "    features.append(hub_score_diff)\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_edge_features(graph, edges):\n",
    "    features = []\n",
    "    for (i, j) in edges:\n",
    "        edge_features = getFeature(graph, i, j)\n",
    "        features.append(edge_features)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for the edges in the data set\n",
    "edges = list(G.edges())\n",
    "X = extract_edge_features(G, edges)\n",
    "y = [1] * len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.16800000e+03, 1.60000000e+01, 1.63265306e-01, ...,\n",
       "        2.38000000e+02, 2.84267622e-01, 1.96612198e-03],\n",
       "       [9.60000000e+02, 3.00000000e+00, 4.61538462e-02, ...,\n",
       "        1.58000000e+02, 9.68245837e-02, 3.12608744e-03],\n",
       "       [3.40800000e+03, 1.50000000e+01, 1.44230769e-01, ...,\n",
       "        2.23000000e+02, 2.56945766e-01, 2.10118749e-03],\n",
       "       ...,\n",
       "       [3.50000000e+01, 2.00000000e+00, 2.00000000e-01, ...,\n",
       "        3.60000000e+01, 3.38061702e-01, 5.79988225e-05],\n",
       "       [2.10000000e+01, 1.00000000e+00, 1.11111111e-01, ...,\n",
       "        1.90000000e+01, 2.18217890e-01, 1.04728736e-04],\n",
       "       [5.00000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.10000000e+01, 0.00000000e+00, 1.00715401e-04]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative samples for the data set\n",
    "def generate_negative_samples(graph, num_samples):\n",
    "    nodes = list(graph.nodes())\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        u, v = np.random.choice(nodes, size=2, replace=False)\n",
    "        if not graph.has_edge(u, v):\n",
    "            negative_samples.append((u, v))\n",
    "    return negative_samples\n",
    "\n",
    "num_negative_samples = int(len(edges)*1.00)  # % of negative samples\n",
    "negative_edges = generate_negative_samples(G, num_negative_samples)\n",
    "X_negative = extract_edge_features(G, negative_edges)\n",
    "y_negative = [0] * len(negative_edges)\n",
    "\n",
    "# Combine positive and negative samples for data set\n",
    "X = np.vstack((X, X_negative))\n",
    "y = np.concatenate((y, y_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.62000000e+02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        7.50000000e+01, 0.00000000e+00, 4.57052642e-04],\n",
       "       [5.00000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.60000000e+01, 0.00000000e+00, 1.15746189e-04],\n",
       "       [2.40000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.40000000e+01, 0.00000000e+00, 1.63108613e-04],\n",
       "       ...,\n",
       "       [7.00000000e+01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.50000000e+01, 0.00000000e+00, 5.90660100e-04],\n",
       "       [4.80000000e+01, 1.00000000e+00, 7.69230769e-02, ...,\n",
       "        1.60000000e+01, 1.44337567e-01, 3.91658766e-06],\n",
       "       [8.50000000e+01, 1.00000000e+00, 4.76190476e-02, ...,\n",
       "        2.20000000e+01, 1.08465229e-01, 8.27593762e-04]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, penalty='l2', C=1.5, solver='lbfgs', max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=5)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.90 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest model\n",
    "rf_clf = RandomForestClassifier(random_state=0, n_estimators=100, max_depth=10)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "rf_scores = cross_val_score(rf_clf, X_test, y_test, cv=5)\n",
    "print(\"Random Forest: %0.2f accuracy with a standard deviation of %0.2f\" % (rf_scores.mean(), rf_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.89 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNN model\n",
    "knn_clf = KNeighborsClassifier(n_neighbors= 20)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "knn_scores = cross_val_score(knn_clf, X_test, y_test, cv=5)\n",
    "print(\"KNN: %0.2f accuracy with a standard deviation of %0.2f\" % (knn_scores.mean(), knn_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting: 0.90 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting model\n",
    "gb_clf = GradientBoostingClassifier(random_state=0, n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "gb_scores = cross_val_score(gb_clf, X_test, y_test, cv=5)\n",
    "print(\"Gradient Boosting: %0.2f accuracy with a standard deviation of %0.2f\" % (gb_scores.mean(), gb_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 15 features, but RandomForestClassifier is expecting 14 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [219]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m solution_edges \u001b[38;5;241m=\u001b[39m df_solutionInput[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint2\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      2\u001b[0m X_solution \u001b[38;5;241m=\u001b[39m extract_edge_features(G, solution_edges)\n\u001b[1;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrf_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_solution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df_solutionInput[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#df_solutionInput.to_csv('predicted_solution.csv', index=False)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:832\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    814\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 832\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    835\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:874\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    872\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    873\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 874\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    877\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:605\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    604\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 605\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 15 features, but RandomForestClassifier is expecting 14 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "solution_edges = df_solutionInput[['int1', 'int2']].astype(str).values.tolist()\n",
    "X_solution = extract_edge_features(G, solution_edges)\n",
    "\n",
    "predictions = rf_clf.predict(X_solution)\n",
    "\n",
    "df_solutionInput['prediction'] = predictions\n",
    "\n",
    "#df_solutionInput.to_csv('predicted_solution.csv', index=False)\n",
    "\n",
    "df_solutionInput.head()\n",
    "\n",
    "num_positive_predictions = df_solutionInput['prediction'].sum()\n",
    "\n",
    "print(f\"Number of positive predictions: {num_positive_predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predictions: 643\n"
     ]
    }
   ],
   "source": [
    "predictions = rf_clf.predict(X_solution)\n",
    "\n",
    "df_solutionInput['prediction'] = predictions\n",
    "\n",
    "#df_solutionInput.to_csv('predicted_solution.csv', index=False)\n",
    "\n",
    "df_solutionInput.head()\n",
    "\n",
    "num_positive_predictions = df_solutionInput['prediction'].sum()\n",
    "\n",
    "print(f\"Number of positive predictions: {num_positive_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities instead of class predictions\n",
    "prediction_probabilities = knn_clf.predict_proba(X_solution)\n",
    "\n",
    "# Get the probabilities for the positive class (class 1)\n",
    "positive_class_probabilities = prediction_probabilities[:, 1]\n",
    "\n",
    "# Find the threshold that gives exactly 50% positive predictions\n",
    "# This is the median of the probabilities\n",
    "threshold = np.median(positive_class_probabilities)\n",
    "\n",
    "# Make predictions using this threshold\n",
    "adjusted_predictions = (positive_class_probabilities >= threshold).astype(int)\n",
    "\n",
    "# Update the predictions in your DataFrame\n",
    "df_solutionInput['prediction'] = adjusted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_solutionInput['prediction'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solutionInput[[\"prediction\"]].to_csv(\"./attempt_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
